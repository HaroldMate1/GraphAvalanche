"""
GraphAvalanche - Citation Network Visualizer for AVALANCHE Results
Works with any Excel output from AVALANCHE literature discovery tool
"""

import streamlit as st
import streamlit.components.v1 as components
import networkx as nx
import plotly.graph_objects as go
import pandas as pd
import requests
import time
from typing import Dict, List, Tuple
import hashlib

# Page configuration
st.set_page_config(
    page_title="GraphAvalanche - Citation Network Visualizer",
    page_icon="üìä",
    layout="wide"
)

# Title and description
st.title("üìä GraphAvalanche - Citation Network Visualizer")
st.markdown("""
**Visualize citation relationships from AVALANCHE literature discovery results**

Upload your AVALANCHE Excel output to create an interactive, Connected Papers-style citation network
with automatic tier highlighting based on citation impact.
""")

# Sidebar controls
st.sidebar.header("üéõÔ∏è Visualization Controls")

# File upload
uploaded_file = st.sidebar.file_uploader(
    "Upload AVALANCHE Excel Results",
    type=['xlsx', 'xls'],
    help="Upload the Excel file generated by AVALANCHE"
)

# Tier thresholds
st.sidebar.subheader("üìä Citation Tier Thresholds")
tier1_threshold = st.sidebar.number_input("Tier 1 (Gold) - Min Citations", min_value=1, value=100, step=10)
tier2_threshold = st.sidebar.number_input("Tier 2 (Silver) - Min Citations", min_value=1, value=50, step=10)
tier3_threshold = st.sidebar.number_input("Tier 3 (Bronze) - Min Citations", min_value=1, value=40, step=10)

# Visualization controls
show_labels = st.sidebar.checkbox("Show paper labels", value=False)
layout_algorithm = st.sidebar.selectbox(
    "Layout Algorithm",
    ["spring", "kamada_kawai", "circular", "spectral"],
    index=0,
    help="Choose how to arrange nodes in the graph"
)
edge_opacity = st.sidebar.slider("Edge Opacity", 0.1, 1.0, 0.3, 0.1)
show_timeline = st.sidebar.checkbox("Show Timeline View", value=True)

# Tier colors (customizable)
tier_colors = {
    'Tier 1': '#FFD700',  # Gold
    'Tier 2': '#C0C0C0',  # Silver
    'Tier 3': '#CD7F32',  # Bronze
    'Other': '#87CEEB'    # Sky Blue
}

tier_sizes = {
    'Tier 1': 40,
    'Tier 2': 30,
    'Tier 3': 25,
    'Other': 20
}


def normalize_doi(doi: str) -> str:
    """Normalize DOI string."""
    if pd.isna(doi) or not doi:
        return ""
    d = str(doi).lower().strip()
    d = d.replace("https://doi.org/", "").replace("doi:", "").replace("http://dx.doi.org/", "")
    return d


def generate_paper_id(row: pd.Series) -> str:
    """Generate unique ID for a paper."""
    if pd.notna(row.get('DOI')) and row.get('DOI'):
        return f"paper_{hashlib.md5(str(row['DOI']).encode()).hexdigest()[:8]}"
    else:
        return f"paper_{hashlib.md5(str(row.get('Title', '')).encode()).hexdigest()[:8]}"


def assign_tier(cited_by: int, t1: int, t2: int, t3: int) -> str:
    """Assign tier based on citation count."""
    if cited_by >= t1:
        return 'Tier 1'
    elif cited_by >= t2:
        return 'Tier 2'
    elif cited_by >= t3:
        return 'Tier 3'
    else:
        return 'Other'


def calculate_proportional_sizes(cited_by_series: pd.Series, min_size: int = 15, max_size: int = 60) -> pd.Series:
    """Calculate node sizes proportional to citation counts."""
    min_citations = cited_by_series.min()
    max_citations = cited_by_series.max()

    if max_citations == min_citations:
        # All same citations, use middle size
        return pd.Series([( min_size + max_size) // 2] * len(cited_by_series), index=cited_by_series.index)

    # Linear scaling between min_size and max_size
    sizes = min_size + (cited_by_series - min_citations) / (max_citations - min_citations) * (max_size - min_size)
    return sizes.astype(int)


def extract_first_author_label(authors_str: str) -> str:
    """Extract 'First Author et al.' from authors string. Returns empty string if unknown."""
    if pd.isna(authors_str) or not authors_str or authors_str == 'Unknown':
        return ''  # Return empty instead of 'Unknown'

    authors_str = str(authors_str).strip()

    # Handle different separators
    if ';' in authors_str:
        authors = authors_str.split(';')
    elif ',' in authors_str and authors_str.count(',') > 1:
        # Multiple commas likely means multiple authors
        authors = authors_str.split(',')
    else:
        authors = [authors_str]

    first_author = authors[0].strip()

    # Extract last name (handle "Last, First" or "First Last" formats)
    if ',' in first_author:
        last_name = first_author.split(',')[0].strip()
    else:
        parts = first_author.split()
        last_name = parts[-1] if parts else first_author

    # Truncate if too long
    if len(last_name) > 12:
        last_name = last_name[:10] + '..'

    if len(authors) > 1:
        return f"{last_name} et al."
    return last_name


def prepare_avalanche_data(df: pd.DataFrame, t1: int, t2: int, t3: int) -> pd.DataFrame:
    """Prepare AVALANCHE data for visualization."""

    # Ensure required columns exist
    required_cols = ['Title', 'DOI', 'Cited_By']
    missing_cols = [col for col in required_cols if col not in df.columns]

    if missing_cols:
        st.error(f"Missing required columns: {', '.join(missing_cols)}")
        st.info("Required columns: Title, DOI, Cited_By, Year (optional)")
        return None

    # Generate unique IDs
    df['ID'] = df.apply(generate_paper_id, axis=1)

    # Assign tiers
    df['Tier'] = df['Cited_By'].apply(lambda x: assign_tier(x, t1, t2, t3))

    # Add colors (keep tier-based)
    df['Color'] = df['Tier'].map(tier_colors)

    # Calculate sizes proportional to citations
    df['Size'] = calculate_proportional_sizes(df['Cited_By'])

    # Handle optional Year column
    if 'Year' not in df.columns:
        df['Year'] = 0

    # Handle Authors column - will be populated from OpenAlex if not present
    if 'Authors' not in df.columns:
        df['Authors'] = 'Unknown'

    # Create author label for display
    df['AuthorLabel'] = df['Authors'].apply(extract_first_author_label)

    # Create short title for labels
    df['ShortTitle'] = df['Title'].apply(lambda x: str(x)[:40] + '...' if len(str(x)) > 40 else str(x))

    return df


@st.cache_data
def fetch_citation_data(papers_df: pd.DataFrame, batch_size: int = 50) -> Tuple[Dict, Dict, Dict]:
    """Fetch citation relationships and author data from OpenAlex."""

    # Build DOI index
    doi_to_id = {}
    for _, row in papers_df.iterrows():
        doi = normalize_doi(row.get('DOI'))
        if doi:
            doi_to_id[doi] = row['ID']

    all_dois = list(doi_to_id.keys())
    doi_to_references = {}
    doi_to_authors = {}  # New: store authors for each paper

    # Progress tracking
    progress_bar = st.progress(0)
    status_text = st.empty()

    total_batches = (len(all_dois) + batch_size - 1) // batch_size

    # Batch fetch OpenAlex records
    for i in range(0, len(all_dois), batch_size):
        batch = all_dois[i:i+batch_size]
        batch_num = i // batch_size + 1

        status_text.text(f"Fetching citation data: batch {batch_num}/{total_batches}")
        progress_bar.progress(batch_num / total_batches)

        filter_str = '|'.join([f'https://doi.org/{d}' for d in batch])
        url = f"https://api.openalex.org/works?filter=doi:{filter_str}&per-page={batch_size}"

        try:
            r = requests.get(url, timeout=30)
            if r.status_code == 200:
                results = r.json().get('results', [])
                for rec in results:
                    rec_doi = rec.get('doi')
                    if rec_doi:
                        norm_doi = normalize_doi(rec_doi)
                        doi_to_references[norm_doi] = rec.get('referenced_works', [])
                        # Extract authors
                        authorships = rec.get('authorships', [])
                        if authorships:
                            author_names = []
                            for auth in authorships:
                                author_info = auth.get('author', {})
                                name = author_info.get('display_name', '')
                                if name:
                                    author_names.append(name)
                            doi_to_authors[norm_doi] = '; '.join(author_names) if author_names else 'Unknown'
                        else:
                            doi_to_authors[norm_doi] = 'Unknown'
            time.sleep(1)  # Rate limiting
        except Exception as e:
            st.warning(f"Error fetching batch {batch_num}: {e}")
            continue

    # Fetch DOIs for all referenced works
    openalex_id_to_doi = {}
    all_refs = set()
    for refs in doi_to_references.values():
        all_refs.update(refs)
    all_refs = list(all_refs)

    total_ref_batches = (len(all_refs) + batch_size - 1) // batch_size

    for i in range(0, len(all_refs), batch_size):
        batch = all_refs[i:i+batch_size]
        batch_num = i // batch_size + 1

        status_text.text(f"Resolving references: batch {batch_num}/{total_ref_batches}")
        progress_bar.progress(batch_num / total_ref_batches)

        filter_str = '|'.join(batch)
        url = f"https://api.openalex.org/works?filter=openalex_id:{filter_str}&per-page={batch_size}"

        try:
            r = requests.get(url, timeout=30)
            if r.status_code == 200:
                results = r.json().get('results', [])
                for rec in results:
                    rec_id = rec.get('id')
                    rec_doi = rec.get('doi')
                    if rec_id and rec_doi:
                        openalex_id_to_doi[rec_id] = normalize_doi(rec_doi)
            time.sleep(1)
        except Exception as e:
            st.warning(f"Error resolving batch {batch_num}: {e}")
            continue

    progress_bar.empty()
    status_text.empty()

    return doi_to_references, openalex_id_to_doi, doi_to_authors


def build_citation_network(papers_df: pd.DataFrame, doi_to_references: Dict, openalex_id_to_doi: Dict, doi_to_authors: Dict) -> nx.DiGraph:
    """Build NetworkX directed graph from citation data."""

    # Build DOI to ID mapping and ID to DOI
    doi_to_id = {}
    id_to_doi = {}
    for _, row in papers_df.iterrows():
        doi = normalize_doi(row.get('DOI'))
        if doi:
            doi_to_id[doi] = row['ID']
            id_to_doi[row['ID']] = doi

    # Create graph
    G = nx.DiGraph()

    # Add nodes with attributes (initial, sizes will be recalculated)
    for _, row in papers_df.iterrows():
        doi = normalize_doi(row.get('DOI'))
        # Get authors from OpenAlex data
        authors = doi_to_authors.get(doi, row.get('Authors', 'Unknown'))
        author_label = extract_first_author_label(authors)

        G.add_node(
            row['ID'],
            title=row['Title'],
            year=row['Year'],
            cited_by=row['Cited_By'],
            tier=row['Tier'],
            color=row['Color'],
            size=row['Size'],  # Will be recalculated based on internal citations
            venue=row.get('Venue', ''),
            doi=row.get('DOI', ''),
            authors=authors,
            author_label=author_label
        )

    # Add edges (citations)
    edge_count = 0
    for src_doi, refs in doi_to_references.items():
        src_id = doi_to_id.get(src_doi)
        if not src_id:
            continue

        for ref in refs:
            tgt_doi = openalex_id_to_doi.get(ref)
            if tgt_doi and tgt_doi in doi_to_id:
                tgt_id = doi_to_id[tgt_doi]
                G.add_edge(src_id, tgt_id)
                edge_count += 1

    # Recalculate sizes based on internal citations (in-degree within the network)
    in_degrees = dict(G.in_degree())
    if in_degrees:
        min_deg = min(in_degrees.values())
        max_deg = max(in_degrees.values())
        min_size, max_size = 15, 60

        for node_id in G.nodes():
            deg = in_degrees.get(node_id, 0)
            if max_deg == min_deg:
                new_size = (min_size + max_size) // 2
            else:
                new_size = int(min_size + (deg - min_deg) / (max_deg - min_deg) * (max_size - min_size))
            G.nodes[node_id]['size'] = new_size
            G.nodes[node_id]['internal_citations'] = deg

    return G


def create_network_visualization(G: nx.DiGraph, layout_algo: str, show_labels: bool, edge_alpha: float):
    """Create interactive Plotly visualization of citation network."""

    # Calculate layout
    if layout_algo == "spring":
        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
    elif layout_algo == "kamada_kawai":
        pos = nx.kamada_kawai_layout(G)
    elif layout_algo == "circular":
        pos = nx.circular_layout(G)
    elif layout_algo == "spectral":
        pos = nx.spectral_layout(G)
    else:
        pos = nx.spring_layout(G, seed=42)

    # Create edge traces
    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color=f'rgba(136,136,136,{edge_alpha})'),
        hoverinfo='none',
        mode='lines',
        name='Citations'
    )

    # Create node traces by tier
    tier_traces = []
    tiers = ['Tier 1', 'Tier 2', 'Tier 3', 'Other']

    for tier in tiers:
        tier_nodes = [n for n in G.nodes() if G.nodes[n]['tier'] == tier]
        if not tier_nodes:
            continue

        x_vals = [pos[n][0] for n in tier_nodes]
        y_vals = [pos[n][1] for n in tier_nodes]
        colors = [G.nodes[n]['color'] for n in tier_nodes]
        sizes = [G.nodes[n]['size'] for n in tier_nodes]

        # Create hover text and DOI URLs
        hover_text = []
        doi_urls = []
        for n in tier_nodes:
            node = G.nodes[n]
            authors_display = node.get('authors', '')
            if authors_display and authors_display != 'Unknown':
                authors_display = authors_display[:50] + '...'
            else:
                authors_display = ''

            text = f"<b>{node['title'][:60]}...</b><br>"
            if authors_display:
                text += f"Author: {authors_display}<br>"
            text += f"Year: {node['year']}<br>"
            text += f"Citations (total): {node['cited_by']}<br>"
            text += f"Citations (in network): {node.get('internal_citations', 0)}<br>"
            text += f"Tier: {node['tier']}<br>"
            text += f"Venue: {str(node['venue'])[:50]}<br>"
            text += f"<i>Click to open article</i>"
            hover_text.append(text)

            # Build DOI URL
            doi = node.get('doi', '')
            if doi and not pd.isna(doi):
                doi_str = str(doi)
                doi_url = f"https://doi.org/{normalize_doi(doi_str)}" if not doi_str.startswith('http') else doi_str
            else:
                doi_url = ''
            doi_urls.append(doi_url)

        # Always show author labels on nodes (empty string if unknown)
        author_labels = [G.nodes[n].get('author_label', '') for n in tier_nodes]

        node_trace = go.Scatter(
            x=x_vals, y=y_vals,
            mode='markers+text',  # Always show text labels
            text=author_labels,
            textposition="top center",
            textfont=dict(size=9, color='#333333'),
            hovertext=hover_text,
            hoverinfo='text',
            customdata=doi_urls,  # Store DOI URLs for click handling
            marker=dict(
                size=sizes,
                color=colors,
                line=dict(width=2, color='white'),
                opacity=0.9
            ),
            name=tier
        )
        tier_traces.append(node_trace)

    # Create figure
    fig = go.Figure(
        data=[edge_trace] + tier_traces,
        layout=go.Layout(
            title=dict(text="Citation Network (Click nodes to open articles)", font=dict(size=16)),
            showlegend=True,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            plot_bgcolor='rgba(240,240,240,0.5)',
            height=700,
            clickmode='event'
        )
    )

    return fig, G  # Return graph for click handling


def create_timeline_visualization(papers_df: pd.DataFrame, G: nx.DiGraph = None):
    """Create horizontal timeline view similar to research milestone timelines.
    Papers alternate above/below the timeline axis with connecting lines.
    Optimized for better spacing and readability.
    """
    df_sorted = papers_df.sort_values('Year').copy()

    # If we have graph data, use internal citation counts for sizing and get author labels
    if G is not None:
        node_data = {n: G.nodes[n] for n in G.nodes()}
        new_sizes = []
        author_labels = []
        doi_urls = []
        for _, row in df_sorted.iterrows():
            paper_id = row['ID']
            if paper_id in node_data:
                new_sizes.append(node_data[paper_id].get('size', row['Size']))
                label = node_data[paper_id].get('author_label', '')
                author_labels.append(label if label != 'Unknown' else '')
            else:
                new_sizes.append(row['Size'])
                label = row.get('AuthorLabel', '')
                author_labels.append(label if label != 'Unknown' else '')

            # Build DOI URL
            doi = row.get('DOI', '')
            if doi and not pd.isna(doi):
                doi_str = str(doi)
                doi_url = f"https://doi.org/{normalize_doi(doi_str)}" if not doi_str.startswith('http') else doi_str
            else:
                doi_url = ''
            doi_urls.append(doi_url)

        df_sorted['Size'] = new_sizes
        df_sorted['AuthorLabel'] = author_labels
        df_sorted['DOI_URL'] = doi_urls

    fig = go.Figure()

    # Get unique years and create positions with more horizontal spacing
    years = sorted(df_sorted['Year'].unique())
    # Increase horizontal spacing between years
    x_spacing = 2.0  # Multiply x positions for more horizontal space
    year_to_x = {year: i * x_spacing for i, year in enumerate(years)}

    # Assign alternating positions (above/below) for papers in each year
    df_sorted['x_pos'] = df_sorted['Year'].map(year_to_x)

    # Group by year and assign y positions with MUCH MORE vertical spacing
    # Use staggered positions to avoid overlap
    y_positions = []
    x_offsets = []  # Small horizontal offsets to spread papers within same year
    y_counter = {}

    # Vertical spacing multiplier - increase this for more space between circles
    vertical_spacing = 1.8

    for idx, (_, row) in enumerate(df_sorted.iterrows()):
        year = row['Year']
        if year not in y_counter:
            y_counter[year] = {'above': 0, 'below': 0, 'next': 'above'}

        if y_counter[year]['next'] == 'above':
            y_counter[year]['above'] += 1
            y_pos = y_counter[year]['above'] * vertical_spacing
            y_counter[year]['next'] = 'below'
        else:
            y_counter[year]['below'] += 1
            y_pos = -y_counter[year]['below'] * vertical_spacing
            y_counter[year]['next'] = 'above'

        y_positions.append(y_pos)

        # Add small horizontal offset for papers in same year to reduce overlap
        papers_in_year = y_counter[year]['above'] + y_counter[year]['below']
        x_offset = (papers_in_year % 3 - 1) * 0.15  # Slight left/center/right variation
        x_offsets.append(x_offset)

    df_sorted['y_pos'] = y_positions
    df_sorted['x_offset'] = x_offsets

    # Draw the main timeline axis (horizontal line at y=0)
    fig.add_trace(go.Scatter(
        x=[i * x_spacing for i in range(len(years))],
        y=[0] * len(years),
        mode='lines+markers',
        line=dict(color='#444444', width=4),
        marker=dict(size=10, color='#444444', symbol='diamond'),
        hoverinfo='skip',
        showlegend=False
    ))

    # Add year labels on the timeline
    for i, year in enumerate(years):
        fig.add_annotation(
            x=i * x_spacing, y=0,
            text=f"<b>{year}</b>",
            showarrow=False,
            yshift=-30,
            font=dict(size=10, color='#333333', family='Arial')
        )

    # Draw connecting lines from timeline to each paper bubble
    for _, row in df_sorted.iterrows():
        x_with_offset = row['x_pos'] + row['x_offset']
        fig.add_trace(go.Scatter(
            x=[row['x_pos'], x_with_offset],
            y=[0, row['y_pos']],
            mode='lines',
            line=dict(color=row['Color'], width=2, dash='dot'),
            hoverinfo='skip',
            showlegend=False
        ))

    # Add paper bubbles by tier (for legend grouping)
    for tier in ['Tier 1', 'Tier 2', 'Tier 3', 'Other']:
        tier_df = df_sorted[df_sorted['Tier'] == tier]
        if len(tier_df) == 0:
            continue

        # Use y_pos directly (already has proper spacing)
        y_vals = tier_df['y_pos'].tolist()
        # Apply x offsets
        x_vals = (tier_df['x_pos'] + tier_df['x_offset']).tolist()

        # Build hover text and labels
        hover_texts = []
        labels = []
        doi_urls = []
        for _, row in tier_df.iterrows():
            author = row.get('AuthorLabel', '')

            # Create cleaner label: just "Author et al." on one line
            if author:
                label = f"{author}"
            else:
                # Use very short title if no author
                title_short = str(row['Title'])[:15] + '...'
                label = title_short
            labels.append(label)

            # Hover text with full details
            hover = f"<b>{row['Title'][:80]}...</b><br>"
            if author:
                hover += f"Author: {author}<br>"
            hover += f"Year: {row['Year']}<br>"
            hover += f"Citations: {row['Cited_By']}<br>"
            hover += f"Tier: {row['Tier']}<br>"
            hover += f"<i>Click to open article</i>"
            hover_texts.append(hover)

            doi_urls.append(row.get('DOI_URL', ''))

        # Determine text positions based on y values
        text_positions = []
        for y in y_vals:
            if y > 0:
                text_positions.append('top center')
            else:
                text_positions.append('bottom center')

        fig.add_trace(go.Scatter(
            x=x_vals,
            y=y_vals,
            mode='markers+text',
            marker=dict(
                size=tier_df['Size'],
                color=tier_df['Color'],
                line=dict(width=2, color='white'),
                opacity=0.9
            ),
            text=labels,
            textposition=text_positions,
            textfont=dict(size=9, color='#333333', family='Arial'),
            hovertext=hover_texts,
            hoverinfo='text',
            customdata=doi_urls,
            name=tier
        ))

    # Calculate axis ranges with padding
    max_x = max(year_to_x.values()) if year_to_x else 1
    max_y = max(abs(min(y_positions)), abs(max(y_positions))) if y_positions else 1

    # Update layout for horizontal timeline appearance
    fig.update_layout(
        title=dict(
            text="Research Timeline (Click bubbles to open articles)",
            font=dict(size=16, family='Arial')
        ),
        xaxis=dict(
            showgrid=False,
            zeroline=False,
            showticklabels=False,
            range=[-1, max_x + 1]
        ),
        yaxis=dict(
            showgrid=False,
            zeroline=False,
            showticklabels=False,
            range=[-(max_y + 1.5), max_y + 1.5]
        ),
        plot_bgcolor='#fafafa',
        paper_bgcolor='white',
        height=800,  # Taller for better spacing
        showlegend=True,
        legend=dict(
            orientation='h',
            yanchor='bottom',
            y=1.02,
            xanchor='center',
            x=0.5,
            font=dict(size=11)
        ),
        hovermode='closest',
        clickmode='event',
        margin=dict(l=20, r=20, t=60, b=60)
    )

    return fig


# Main app logic
if uploaded_file is not None:
    try:
        # Load data
        with st.spinner("Loading AVALANCHE data..."):
            df = pd.read_excel(uploaded_file)

        # Prepare data
        df_prepared = prepare_avalanche_data(df, tier1_threshold, tier2_threshold, tier3_threshold)

        if df_prepared is not None:
            st.success(f"‚úÖ Loaded {len(df_prepared)} papers")

            # Display tier statistics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(f"Tier 1 (‚â•{tier1_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 1']))
            with col2:
                st.metric(f"Tier 2 (‚â•{tier2_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 2']))
            with col3:
                st.metric(f"Tier 3 (‚â•{tier3_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 3']))
            with col4:
                st.metric(f"Other (<{tier3_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Other']))

            # Fetch citation data
            if st.button("üîÑ Fetch Citation Network (may take 2-3 minutes)"):
                with st.spinner("Fetching citation relationships and author data from OpenAlex..."):
                    doi_to_refs, oa_id_to_doi, doi_to_authors = fetch_citation_data(df_prepared)

                st.success(f"‚úÖ Fetched citation data for {len(doi_to_refs)} papers")

                # Build network
                with st.spinner("Building citation network..."):
                    G = build_citation_network(df_prepared, doi_to_refs, oa_id_to_doi, doi_to_authors)

                st.success(f"‚úÖ Network built: {len(G.nodes())} nodes, {len(G.edges())} edges")

                # Store in session state
                st.session_state['graph'] = G
                st.session_state['df'] = df_prepared

            # Visualize if graph exists
            if 'graph' in st.session_state:
                G = st.session_state['graph']
                df_viz = st.session_state['df']

                st.markdown("---")
                st.header("üìà Citation Network Graph")

                # Network statistics
                with st.expander("üìä Network Statistics"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total Papers", len(G.nodes()))
                    with col2:
                        st.metric("Citation Links", len(G.edges()))
                    with col3:
                        density = nx.density(G)
                        st.metric("Network Density", f"{density:.4f}")

                    # Most cited within network
                    in_degrees = dict(G.in_degree())
                    if in_degrees:
                        most_cited_id = max(in_degrees, key=in_degrees.get)
                        most_cited_node = G.nodes[most_cited_id]
                        st.write(f"**Most cited in network:** {most_cited_node['title'][:60]}... ({in_degrees[most_cited_id]} citations within this network)")

                # Create visualization
                with st.spinner("Rendering network visualization..."):
                    fig_network, _ = create_network_visualization(G, layout_algorithm, show_labels, edge_opacity)

                # Add JavaScript for click handling to open DOI links
                st.markdown("""
                <script>
                document.addEventListener('plotly_click', function(data) {
                    if (data.points && data.points[0] && data.points[0].customdata) {
                        var url = data.points[0].customdata;
                        if (url) window.open(url, '_blank');
                    }
                });
                </script>
                """, unsafe_allow_html=True)

                # Display network chart with click events
                selected_point = st.plotly_chart(fig_network, use_container_width=True, on_select="rerun", key="network_chart")

                # Handle click events for network
                if selected_point and 'selection' in selected_point and selected_point['selection'].get('points'):
                    point = selected_point['selection']['points'][0]
                    if 'customdata' in point and point['customdata']:
                        doi_url = point['customdata']
                        st.markdown(f"**Selected paper:** [Open in browser]({doi_url})")
                        st.components.v1.html(f'<script>window.open("{doi_url}", "_blank");</script>', height=0)

                # Timeline view
                if show_timeline:
                    st.markdown("---")
                    st.header("üìÖ Timeline View")
                    fig_timeline = create_timeline_visualization(df_viz, G)

                    selected_timeline = st.plotly_chart(fig_timeline, use_container_width=True, on_select="rerun", key="timeline_chart")

                    # Handle click events for timeline
                    if selected_timeline and 'selection' in selected_timeline and selected_timeline['selection'].get('points'):
                        point = selected_timeline['selection']['points'][0]
                        if 'customdata' in point and point['customdata']:
                            doi_url = point['customdata']
                            st.markdown(f"**Selected paper:** [Open in browser]({doi_url})")
                            st.components.v1.html(f'<script>window.open("{doi_url}", "_blank");</script>', height=0)

                # Export options
                st.markdown("---")
                st.header("üíæ Export Options")

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üì• Download Network Data (GraphML)"):
                        import tempfile
                        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.graphml') as f:
                            nx.write_graphml(G, f.name)
                            st.success(f"Network saved to: {f.name}")

                with col2:
                    csv = df_viz.to_csv(index=False)
                    st.download_button(
                        label="üì• Download CSV",
                        data=csv,
                        file_name="citation_network_data.csv",
                        mime="text/csv"
                    )
            else:
                st.info("üëÜ Click 'Fetch Citation Network' to build the visualization")

                # Show sample data
                st.markdown("---")
                st.header("üìã Data Preview")
                st.dataframe(df_prepared[['Title', 'Year', 'Cited_By', 'Tier', 'Venue']].head(10) if 'Venue' in df_prepared.columns else df_prepared[['Title', 'Year', 'Cited_By', 'Tier']].head(10))

    except Exception as e:
        st.error(f"‚ùå Error processing file: {e}")
        import traceback
        st.code(traceback.format_exc())

else:
    # Instructions when no file uploaded
    st.info("üëà Upload your AVALANCHE Excel results using the sidebar to get started")

    st.markdown("""
    ### üìñ How to Use GraphAvalanche

    1. **Run AVALANCHE** to generate your literature discovery results
    2. **Upload** the Excel file using the sidebar
    3. **Customize** tier thresholds if needed (default: Tier 1‚â•100, Tier 2‚â•50, Tier 3‚â•40 citations)
    4. **Click** "Fetch Citation Network" to build the visualization
    5. **Explore** your citation network with interactive controls
    6. **Export** GraphML or CSV for further analysis

    ### üìä Features

    - **Tier-based highlighting**: Automatically categorizes papers by citation impact
    - **Interactive network graph**: Zoom, pan, hover for details
    - **Multiple layouts**: Choose from spring, kamada-kawai, circular, or spectral
    - **Timeline view**: See publication trends over time
    - **Export options**: GraphML for Gephi/Cytoscape, CSV for data analysis

    ### üìã Required Excel Columns

    Your AVALANCHE output should have:
    - **Title**: Paper title
    - **DOI**: Digital Object Identifier
    - **Cited_By**: Citation count
    - **Year**: Publication year (optional)
    - **Venue**: Journal/conference (optional)

    ### üé® Citation Tiers

    Papers are color-coded by citation impact:
    - üü° **Tier 1 (Gold)**: High-impact papers (default: ‚â•100 citations)
    - ‚ö™ **Tier 2 (Silver)**: Important papers (default: ‚â•50 citations)
    - üü§ **Tier 3 (Bronze)**: Core papers (default: ‚â•40 citations)
    - üîµ **Other (Blue)**: Supporting literature (<40 citations)
    """)

    # Example data structure
    st.markdown("---")
    st.subheader("üìÑ Example AVALANCHE Output Format")

    example_data = pd.DataFrame({
        'Title': ['Example Paper 1', 'Example Paper 2', 'Example Paper 3'],
        'Year': [2020, 2019, 2021],
        'DOI': ['10.1234/example1', '10.1234/example2', '10.1234/example3'],
        'Cited_By': [150, 75, 45],
        'Venue': ['Nature', 'Science', 'Cell'],
        'Abstract': ['Abstract text...', 'Abstract text...', 'Abstract text...']
    })

    st.dataframe(example_data)
