"""
GraphAvalanche - Citation Network Visualizer for AVALANCHE Results
Works with any Excel output from AVALANCHE literature discovery tool
"""

import streamlit as st
import networkx as nx
import plotly.graph_objects as go
import pandas as pd
import requests
import time
from typing import Dict, List, Tuple
import hashlib

# Page configuration
st.set_page_config(
    page_title="GraphAvalanche - Citation Network Visualizer",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Title and description
st.title("ðŸ“Š GraphAvalanche - Citation Network Visualizer")
st.markdown("""
**Visualize citation relationships from AVALANCHE literature discovery results**

Upload your AVALANCHE Excel output to create an interactive, Connected Papers-style citation network
with automatic tier highlighting based on citation impact.
""")

# Sidebar controls
st.sidebar.header("ðŸŽ›ï¸ Visualization Controls")

# File upload
uploaded_file = st.sidebar.file_uploader(
    "Upload AVALANCHE Excel Results",
    type=['xlsx', 'xls'],
    help="Upload the Excel file generated by AVALANCHE"
)

# Tier thresholds
st.sidebar.subheader("ðŸ“Š Citation Tier Thresholds")
tier1_threshold = st.sidebar.number_input("Tier 1 (Gold) - Min Citations", min_value=1, value=100, step=10)
tier2_threshold = st.sidebar.number_input("Tier 2 (Silver) - Min Citations", min_value=1, value=50, step=10)
tier3_threshold = st.sidebar.number_input("Tier 3 (Bronze) - Min Citations", min_value=1, value=40, step=10)

# Visualization controls
show_labels = st.sidebar.checkbox("Show paper labels", value=False)
layout_algorithm = st.sidebar.selectbox(
    "Layout Algorithm",
    ["spring", "kamada_kawai", "circular", "spectral"],
    index=0,
    help="Choose how to arrange nodes in the graph"
)
edge_opacity = st.sidebar.slider("Edge Opacity", 0.1, 1.0, 0.3, 0.1)
show_timeline = st.sidebar.checkbox("Show Timeline View", value=True)

# Tier colors (customizable)
tier_colors = {
    'Tier 1': '#FFD700',  # Gold
    'Tier 2': '#C0C0C0',  # Silver
    'Tier 3': '#CD7F32',  # Bronze
    'Other': '#87CEEB'    # Sky Blue
}

tier_sizes = {
    'Tier 1': 40,
    'Tier 2': 30,
    'Tier 3': 25,
    'Other': 20
}


def normalize_doi(doi: str) -> str:
    """Normalize DOI string."""
    if pd.isna(doi) or not doi:
        return ""
    d = str(doi).lower().strip()
    d = d.replace("https://doi.org/", "").replace("doi:", "").replace("http://dx.doi.org/", "")
    return d


def generate_paper_id(row: pd.Series) -> str:
    """Generate unique ID for a paper."""
    if pd.notna(row.get('DOI')) and row.get('DOI'):
        return f"paper_{hashlib.md5(str(row['DOI']).encode()).hexdigest()[:8]}"
    else:
        return f"paper_{hashlib.md5(str(row.get('Title', '')).encode()).hexdigest()[:8]}"


def assign_tier(cited_by: int, t1: int, t2: int, t3: int) -> str:
    """Assign tier based on citation count."""
    if cited_by >= t1:
        return 'Tier 1'
    elif cited_by >= t2:
        return 'Tier 2'
    elif cited_by >= t3:
        return 'Tier 3'
    else:
        return 'Other'


def prepare_avalanche_data(df: pd.DataFrame, t1: int, t2: int, t3: int) -> pd.DataFrame:
    """Prepare AVALANCHE data for visualization."""

    # Ensure required columns exist
    required_cols = ['Title', 'DOI', 'Cited_By']
    missing_cols = [col for col in required_cols if col not in df.columns]

    if missing_cols:
        st.error(f"Missing required columns: {', '.join(missing_cols)}")
        st.info("Required columns: Title, DOI, Cited_By, Year (optional)")
        return None

    # Generate unique IDs
    df['ID'] = df.apply(generate_paper_id, axis=1)

    # Assign tiers
    df['Tier'] = df['Cited_By'].apply(lambda x: assign_tier(x, t1, t2, t3))

    # Add colors and sizes
    df['Color'] = df['Tier'].map(tier_colors)
    df['Size'] = df['Tier'].map(tier_sizes)

    # Handle optional Year column
    if 'Year' not in df.columns:
        df['Year'] = 0

    # Create short title for labels
    df['ShortTitle'] = df['Title'].apply(lambda x: str(x)[:40] + '...' if len(str(x)) > 40 else str(x))

    return df


@st.cache_data
def fetch_citation_data(papers_df: pd.DataFrame, batch_size: int = 50) -> Tuple[Dict, Dict]:
    """Fetch citation relationships from OpenAlex."""

    # Build DOI index
    doi_to_id = {}
    for _, row in papers_df.iterrows():
        doi = normalize_doi(row.get('DOI'))
        if doi:
            doi_to_id[doi] = row['ID']

    all_dois = list(doi_to_id.keys())
    doi_to_references = {}

    # Progress tracking
    progress_bar = st.progress(0)
    status_text = st.empty()

    total_batches = (len(all_dois) + batch_size - 1) // batch_size

    # Batch fetch OpenAlex records
    for i in range(0, len(all_dois), batch_size):
        batch = all_dois[i:i+batch_size]
        batch_num = i // batch_size + 1

        status_text.text(f"Fetching citation data: batch {batch_num}/{total_batches}")
        progress_bar.progress(batch_num / total_batches)

        filter_str = '|'.join([f'https://doi.org/{d}' for d in batch])
        url = f"https://api.openalex.org/works?filter=doi:{filter_str}&per-page={batch_size}"

        try:
            r = requests.get(url, timeout=30)
            if r.status_code == 200:
                results = r.json().get('results', [])
                for rec in results:
                    rec_doi = rec.get('doi')
                    if rec_doi:
                        norm_doi = normalize_doi(rec_doi)
                        doi_to_references[norm_doi] = rec.get('referenced_works', [])
            time.sleep(1)  # Rate limiting
        except Exception as e:
            st.warning(f"Error fetching batch {batch_num}: {e}")
            continue

    # Fetch DOIs for all referenced works
    openalex_id_to_doi = {}
    all_refs = set()
    for refs in doi_to_references.values():
        all_refs.update(refs)
    all_refs = list(all_refs)

    total_ref_batches = (len(all_refs) + batch_size - 1) // batch_size

    for i in range(0, len(all_refs), batch_size):
        batch = all_refs[i:i+batch_size]
        batch_num = i // batch_size + 1

        status_text.text(f"Resolving references: batch {batch_num}/{total_ref_batches}")
        progress_bar.progress(batch_num / total_ref_batches)

        filter_str = '|'.join(batch)
        url = f"https://api.openalex.org/works?filter=openalex_id:{filter_str}&per-page={batch_size}"

        try:
            r = requests.get(url, timeout=30)
            if r.status_code == 200:
                results = r.json().get('results', [])
                for rec in results:
                    rec_id = rec.get('id')
                    rec_doi = rec.get('doi')
                    if rec_id and rec_doi:
                        openalex_id_to_doi[rec_id] = normalize_doi(rec_doi)
            time.sleep(1)
        except Exception as e:
            st.warning(f"Error resolving batch {batch_num}: {e}")
            continue

    progress_bar.empty()
    status_text.empty()

    return doi_to_references, openalex_id_to_doi


def build_citation_network(papers_df: pd.DataFrame, doi_to_references: Dict, openalex_id_to_doi: Dict) -> nx.DiGraph:
    """Build NetworkX directed graph from citation data."""

    # Build DOI to ID mapping
    doi_to_id = {}
    for _, row in papers_df.iterrows():
        doi = normalize_doi(row.get('DOI'))
        if doi:
            doi_to_id[doi] = row['ID']

    # Create graph
    G = nx.DiGraph()

    # Add nodes with attributes
    for _, row in papers_df.iterrows():
        G.add_node(
            row['ID'],
            title=row['Title'],
            year=row['Year'],
            cited_by=row['Cited_By'],
            tier=row['Tier'],
            color=row['Color'],
            size=row['Size'],
            venue=row.get('Venue', ''),
            doi=row.get('DOI', '')
        )

    # Add edges (citations)
    edge_count = 0
    for src_doi, refs in doi_to_references.items():
        src_id = doi_to_id.get(src_doi)
        if not src_id:
            continue

        for ref in refs:
            tgt_doi = openalex_id_to_doi.get(ref)
            if tgt_doi and tgt_doi in doi_to_id:
                tgt_id = doi_to_id[tgt_doi]
                G.add_edge(src_id, tgt_id)
                edge_count += 1

    return G


def create_network_visualization(G: nx.DiGraph, layout_algo: str, show_labels: bool, edge_alpha: float):
    """Create interactive Plotly visualization of citation network."""

    # Calculate layout
    if layout_algo == "spring":
        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
    elif layout_algo == "kamada_kawai":
        pos = nx.kamada_kawai_layout(G)
    elif layout_algo == "circular":
        pos = nx.circular_layout(G)
    elif layout_algo == "spectral":
        pos = nx.spectral_layout(G)
    else:
        pos = nx.spring_layout(G, seed=42)

    # Create edge traces
    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color=f'rgba(136,136,136,{edge_alpha})'),
        hoverinfo='none',
        mode='lines',
        name='Citations'
    )

    # Create node traces by tier
    tier_traces = []
    tiers = ['Tier 1', 'Tier 2', 'Tier 3', 'Other']

    for tier in tiers:
        tier_nodes = [n for n in G.nodes() if G.nodes[n]['tier'] == tier]
        if not tier_nodes:
            continue

        x_vals = [pos[n][0] for n in tier_nodes]
        y_vals = [pos[n][1] for n in tier_nodes]
        colors = [G.nodes[n]['color'] for n in tier_nodes]
        sizes = [G.nodes[n]['size'] for n in tier_nodes]

        # Create hover text
        hover_text = []
        for n in tier_nodes:
            node = G.nodes[n]
            text = f"<b>{node['title'][:60]}...</b><br>"
            text += f"Year: {node['year']}<br>"
            text += f"Citations: {node['cited_by']}<br>"
            text += f"Tier: {node['tier']}<br>"
            text += f"Venue: {str(node['venue'])[:50]}"
            hover_text.append(text)

        # Labels
        labels = [G.nodes[n]['title'][:20] + '...' for n in tier_nodes] if show_labels else None

        node_trace = go.Scatter(
            x=x_vals, y=y_vals,
            mode='markers+text' if show_labels else 'markers',
            text=labels,
            textposition="top center",
            hovertext=hover_text,
            hoverinfo='text',
            marker=dict(
                size=sizes,
                color=colors,
                line=dict(width=2, color='white'),
                opacity=0.9
            ),
            name=tier
        )
        tier_traces.append(node_trace)

    # Create figure
    fig = go.Figure(
        data=[edge_trace] + tier_traces,
        layout=go.Layout(
            title=dict(text="Citation Network (Directed Graph)", font=dict(size=16)),
            showlegend=True,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            plot_bgcolor='rgba(240,240,240,0.5)',
            height=700
        )
    )

    return fig


def create_timeline_visualization(papers_df: pd.DataFrame):
    """Create timeline view of papers."""
    df_sorted = papers_df.sort_values('Year')

    fig = go.Figure()

    # Add traces by tier
    for tier in ['Tier 1', 'Tier 2', 'Tier 3', 'Other']:
        tier_df = df_sorted[df_sorted['Tier'] == tier]
        if len(tier_df) == 0:
            continue

        fig.add_trace(go.Scatter(
            x=tier_df['Year'],
            y=tier_df['Cited_By'],
            mode='markers',
            marker=dict(
                size=tier_df['Size'],
                color=tier_df['Color'],
                line=dict(width=1, color='white'),
                opacity=0.8
            ),
            text=tier_df['Title'].apply(lambda x: str(x)[:50] + '...'),
            hovertemplate='<b>%{text}</b><br>Year: %{x}<br>Citations: %{y}<extra></extra>',
            name=tier
        ))

    fig.update_layout(
        title="Timeline: Papers by Year and Citation Count",
        xaxis_title="Publication Year",
        yaxis_title="Citation Count",
        hovermode='closest',
        height=400,
        showlegend=True
    )

    return fig


# Main app logic
if uploaded_file is not None:
    try:
        # Load data
        with st.spinner("Loading AVALANCHE data..."):
            df = pd.read_excel(uploaded_file)

        # Prepare data
        df_prepared = prepare_avalanche_data(df, tier1_threshold, tier2_threshold, tier3_threshold)

        if df_prepared is not None:
            st.success(f"âœ… Loaded {len(df_prepared)} papers")

            # Display tier statistics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(f"Tier 1 (â‰¥{tier1_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 1']))
            with col2:
                st.metric(f"Tier 2 (â‰¥{tier2_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 2']))
            with col3:
                st.metric(f"Tier 3 (â‰¥{tier3_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Tier 3']))
            with col4:
                st.metric(f"Other (<{tier3_threshold} citations)", len(df_prepared[df_prepared['Tier'] == 'Other']))

            # Fetch citation data
            if st.button("ðŸ”„ Fetch Citation Network (may take 2-3 minutes)"):
                with st.spinner("Fetching citation relationships from OpenAlex..."):
                    doi_to_refs, oa_id_to_doi = fetch_citation_data(df_prepared)

                st.success(f"âœ… Fetched citation data for {len(doi_to_refs)} papers")

                # Build network
                with st.spinner("Building citation network..."):
                    G = build_citation_network(df_prepared, doi_to_refs, oa_id_to_doi)

                st.success(f"âœ… Network built: {len(G.nodes())} nodes, {len(G.edges())} edges")

                # Store in session state
                st.session_state['graph'] = G
                st.session_state['df'] = df_prepared

            # Visualize if graph exists
            if 'graph' in st.session_state:
                G = st.session_state['graph']
                df_viz = st.session_state['df']

                st.markdown("---")
                st.header("ðŸ“ˆ Citation Network Graph")

                # Network statistics
                with st.expander("ðŸ“Š Network Statistics"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total Papers", len(G.nodes()))
                    with col2:
                        st.metric("Citation Links", len(G.edges()))
                    with col3:
                        density = nx.density(G)
                        st.metric("Network Density", f"{density:.4f}")

                    # Most cited within network
                    in_degrees = dict(G.in_degree())
                    if in_degrees:
                        most_cited_id = max(in_degrees, key=in_degrees.get)
                        most_cited_node = G.nodes[most_cited_id]
                        st.write(f"**Most cited in network:** {most_cited_node['title'][:60]}... ({in_degrees[most_cited_id]} citations within this network)")

                # Create visualization
                with st.spinner("Rendering network visualization..."):
                    fig_network = create_network_visualization(G, layout_algorithm, show_labels, edge_opacity)
                st.plotly_chart(fig_network, width='stretch')

                # Timeline view
                if show_timeline:
                    st.markdown("---")
                    st.header("ðŸ“… Timeline View")
                    fig_timeline = create_timeline_visualization(df_viz)
                    st.plotly_chart(fig_timeline, width='stretch')

                # Export options
                st.markdown("---")
                st.header("ðŸ’¾ Export Options")

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ðŸ“¥ Download Network Data (GraphML)"):
                        import tempfile
                        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.graphml') as f:
                            nx.write_graphml(G, f.name)
                            st.success(f"Network saved to: {f.name}")

                with col2:
                    csv = df_viz.to_csv(index=False)
                    st.download_button(
                        label="ðŸ“¥ Download CSV",
                        data=csv,
                        file_name="citation_network_data.csv",
                        mime="text/csv"
                    )
            else:
                st.info("ðŸ‘† Click 'Fetch Citation Network' to build the visualization")

                # Show sample data
                st.markdown("---")
                st.header("ðŸ“‹ Data Preview")
                st.dataframe(df_prepared[['Title', 'Year', 'Cited_By', 'Tier', 'Venue']].head(10) if 'Venue' in df_prepared.columns else df_prepared[['Title', 'Year', 'Cited_By', 'Tier']].head(10))

    except Exception as e:
        st.error(f"âŒ Error processing file: {e}")
        import traceback
        st.code(traceback.format_exc())

else:
    # Instructions when no file uploaded
    st.info("ðŸ‘ˆ Upload your AVALANCHE Excel results using the sidebar to get started")

    st.markdown("""
    ### ðŸ“– How to Use GraphAvalanche

    1. **Run AVALANCHE** to generate your literature discovery results
    2. **Upload** the Excel file using the sidebar
    3. **Customize** tier thresholds if needed (default: Tier 1â‰¥100, Tier 2â‰¥50, Tier 3â‰¥40 citations)
    4. **Click** "Fetch Citation Network" to build the visualization
    5. **Explore** your citation network with interactive controls
    6. **Export** GraphML or CSV for further analysis

    ### ðŸ“Š Features

    - **Tier-based highlighting**: Automatically categorizes papers by citation impact
    - **Interactive network graph**: Zoom, pan, hover for details
    - **Multiple layouts**: Choose from spring, kamada-kawai, circular, or spectral
    - **Timeline view**: See publication trends over time
    - **Export options**: GraphML for Gephi/Cytoscape, CSV for data analysis

    ### ðŸ“‹ Required Excel Columns

    Your AVALANCHE output should have:
    - **Title**: Paper title
    - **DOI**: Digital Object Identifier
    - **Cited_By**: Citation count
    - **Year**: Publication year (optional)
    - **Venue**: Journal/conference (optional)

    ### ðŸŽ¨ Citation Tiers

    Papers are color-coded by citation impact:
    - ðŸŸ¡ **Tier 1 (Gold)**: High-impact papers (default: â‰¥100 citations)
    - âšª **Tier 2 (Silver)**: Important papers (default: â‰¥50 citations)
    - ðŸŸ¤ **Tier 3 (Bronze)**: Core papers (default: â‰¥40 citations)
    - ðŸ”µ **Other (Blue)**: Supporting literature (<40 citations)
    """)

    # Example data structure
    st.markdown("---")
    st.subheader("ðŸ“„ Example AVALANCHE Output Format")

    example_data = pd.DataFrame({
        'Title': ['Example Paper 1', 'Example Paper 2', 'Example Paper 3'],
        'Year': [2020, 2019, 2021],
        'DOI': ['10.1234/example1', '10.1234/example2', '10.1234/example3'],
        'Cited_By': [150, 75, 45],
        'Venue': ['Nature', 'Science', 'Cell'],
        'Abstract': ['Abstract text...', 'Abstract text...', 'Abstract text...']
    })

    st.dataframe(example_data)
